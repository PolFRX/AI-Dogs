{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "save_dataset_pickle.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MbJACn2A4h5-",
        "colab_type": "text"
      },
      "source": [
        "Importations of libs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hADxqIP9JfEs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "from google.colab import files\n",
        "import random\n",
        "import math\n",
        "import os\n",
        "import pickle\n",
        "import tensorflow.keras.models as km\n",
        "import tensorflow.keras.layers as layers\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras.callbacks as call\n",
        "import tensorflow.keras.applications as ka\n",
        "from IPython.display import clear_output\n",
        "import tensorflow.keras.regularizers as kr\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tM7P_MEYKS_H",
        "colab_type": "text"
      },
      "source": [
        "Functions to connect to drive to take the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1LGqQdjyKaBw",
        "colab_type": "code",
        "outputId": "e1210bc3-0903-4f91-8b24-22be9657989c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3nenOYcX4Zcj",
        "colab_type": "text"
      },
      "source": [
        "Verification that it's well connected"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t1flfpgiQvvb",
        "colab_type": "code",
        "outputId": "76513b9e-e77c-4114-8b0d-c7f5e668fc9d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "os.getcwd()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aFOZ00ct4c2m",
        "colab_type": "text"
      },
      "source": [
        "To see what pickle files exist"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1yV1zQcTQMnA",
        "colab_type": "code",
        "outputId": "ccab48fc-3b9c-4008-b60d-7106d4527792",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "path_pickle = os.getcwd() + \"/gdrive/My Drive/dogs_AI/pickles\"\n",
        "os.listdir(path_pickle)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['images_6_1.pickle',\n",
              " 'labels_6_1.pickle',\n",
              " 'images_6_2.pickle',\n",
              " 'labels_6_2.pickle',\n",
              " 'images_6_3.pickle',\n",
              " 'labels_6_3.pickle',\n",
              " 'images_6_4.pickle',\n",
              " 'labels_6_4.pickle']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W9En0sp74Uoi",
        "colab_type": "text"
      },
      "source": [
        "Fonctions for categories: one for all, one for only 6 categories"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U2RWlnOONLcM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# for all breeds\n",
        "def get_categories():\n",
        "    \"\"\" To have all categories names.\n",
        "\n",
        "     Return:\n",
        "         categories: a list of all categories\n",
        "     \"\"\"\n",
        "    # Take the path to directory \"dataset\"\n",
        "    path = os.getcwd()\n",
        "    path = path + \"/gdrive/My Drive/dataset\"\n",
        "    dataset_dirs = os.listdir(path)\n",
        "    # Declare the list which will contain categories names\n",
        "    categories = list()\n",
        "\n",
        "    # Browse the \"dataset\" directory to get all directories names which are the categories\n",
        "    for dirs in dataset_dirs:\n",
        "        categories.append(dirs)\n",
        "\n",
        "    return categories\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zkAgdWII9eFb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# for 6 breeds\n",
        "def get_categories_6_breeds():\n",
        "    \"\"\" To have all categories names.\n",
        "\n",
        "     Return:\n",
        "         categories: a list of all categories\n",
        "     \"\"\"\n",
        "    # Take the path to directory \"dataset\"\n",
        "    path = os.getcwd()\n",
        "    path = path + \"/gdrive/My Drive/dataset\"\n",
        "    dataset_dirs = os.listdir(path)\n",
        "    # Declare the list which will contain categories names\n",
        "    categories = list()\n",
        "\n",
        "    # Browse the \"dataset\" directory to get all directories names which are the categories\n",
        "    for dirs in dataset_dirs:\n",
        "      if (dirs == \"pug\" or dirs == \"English_setter\" or dirs == \"German_shepherd\"\n",
        "                    or dirs == \"golden_retriever\" or dirs == \"collie\" or dirs == \"boxer\"):\n",
        "        categories.append(dirs)\n",
        "\n",
        "    return categories"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6zj4D9sJ4Qjm",
        "colab_type": "text"
      },
      "source": [
        "Load all constants"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1EWE0KNb1jnf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load constants\n",
        "# CATEGORIES = get_categories()  # we take all the categories at first\n",
        "CATEGORIES = get_categories_6_breeds()  # we take all the categories at first\n",
        "SIZE = 250  # size of images: SIZE * SIZE\n",
        "LEARNING_RATE = 5e-8  # the model's learning rate\n",
        "BATCH_SIZE = 1  # the batch size of data to train the model, 36 is good\n",
        "MODEL_NAME = \"dogs_6\"  # the name that will be taken for the model's save\n",
        "GRAY_SCALE = False  # to know if we use grayscale images  to feed the model\n",
        "DOGS_6 = True  # to know if we take only 6 breeds of dogs to test the model or all breeds\n",
        "STEPS_PER_EPOCH = 129  # steps per epoch, 100 is good for large dataset, else use less\n",
        "EPOCHS = 6  # number of epochs to train the model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xlDQNtNe4Kfa",
        "colab_type": "text"
      },
      "source": [
        "Verification that the categories have been loaded"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VlSzTOU493Tb",
        "colab_type": "code",
        "outputId": "a87bb5e5-47ca-4c84-da01-35d9f5255b3b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "CATEGORIES"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['pug',\n",
              " 'English_setter',\n",
              " 'golden_retriever',\n",
              " 'German_shepherd',\n",
              " 'collie',\n",
              " 'boxer']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v8iI7EBuM-Hw",
        "colab_type": "text"
      },
      "source": [
        "Functions to save all dataset in pickle file to avoid preprocessing again"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T9Y0yD_mNRW0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def save_files_cv2(categories, size, gray_scale):\n",
        "    \"\"\" Get files only for 6 categories (golden retriever, english setter,\n",
        "    german shepard, collie, pug) to test if the model works with a few breeds \"\"\"\n",
        "    # Take the path to directory \"dataset\"\n",
        "    path = os.getcwd()\n",
        "    path = path + \"/gdrive/My Drive/dataset\"\n",
        "    path_pickle = os.getcwd() + \"/gdrive/My Drive/dogs_AI/pickles\"\n",
        "    dataset_dirs = os.listdir(path)\n",
        "    # Declare the two list which will contain labels and path of the images\n",
        "    labels = list()\n",
        "    images = list()\n",
        "    \n",
        "    # counter for save batch every 1 000 images\n",
        "    i = 0\n",
        "    j = 1\n",
        "\n",
        "    # Browse the \"dataset\" directory to get all paths for images and labels which are the directories names\n",
        "    for dirs in dataset_dirs:\n",
        "        dir_path = path + \"/\" + dirs\n",
        "        for path2, dir, files in os.walk(dir_path):\n",
        "            for files_name in files:\n",
        "                labels.append(categories.index(dirs))\n",
        "                img = cv2_preprocess(dir_path + \"/\" + files_name, size, gray_scale)\n",
        "                images.append(img.tolist())\n",
        "\n",
        "                # Data augmentation\n",
        "                # flip the image\n",
        "                img_flipped = np.fliplr(img)\n",
        "                labels.append(categories.index(dirs))\n",
        "                images.append(img_flipped.tolist())\n",
        "\n",
        "                if random.random() < 0.6:\n",
        "                    img = data_left_translation(img, size)\n",
        "                    labels.append(categories.index(dirs))\n",
        "                    images.append(img.tolist())\n",
        "                elif random.random() < 0.6:\n",
        "                    img = data_right_translation(img, size)\n",
        "                    labels.append(categories.index(dirs))\n",
        "                    images.append(img.tolist())\n",
        "                    \n",
        "                img = None\n",
        "                    \n",
        "                if i >= 500:\n",
        "                    pickle_out = open(\"{}/images_6_{}.pickle\".format(path_pickle, j), \"wb\")\n",
        "                    pickle.dump(images, pickle_out)\n",
        "                    pickle_out.close()\n",
        "                    \n",
        "                    images = None\n",
        "\n",
        "                    pickle_out = open(\"{}/labels_6_{}.pickle\".format(path_pickle, j), \"wb\")\n",
        "                    pickle.dump(labels, pickle_out)\n",
        "                    pickle_out.close()\n",
        "                    \n",
        "                    labels = None\n",
        "                    \n",
        "                    images = list()\n",
        "                    labels = list()\n",
        "                        \n",
        "                    i = 0\n",
        "                    print(\"end of j == {} in all breeds\".format(j))\n",
        "                    j += 1\n",
        "\n",
        "    if i != 0:\n",
        "      pickle_out = open(\"{}/images_6_{}.pickle\".format(path_pickle, j), \"wb\")\n",
        "      pickle.dump(images, pickle_out)\n",
        "      pickle_out.close()\n",
        "      \n",
        "      images = None\n",
        "\n",
        "      pickle_out = open(\"{}/labels_6_{}.pickle\".format(path_pickle, j), \"wb\")\n",
        "      pickle.dump(labels, pickle_out)\n",
        "      pickle_out.close()\n",
        "      \n",
        "      labels = None\n",
        "                        \n",
        "      i = None\n",
        "      j = None\n",
        "      \n",
        "      print(\" end of all breeds \")\n",
        "\n",
        "\n",
        "def save_files_for_some_categories_cv2(categories, size, gray_scale):\n",
        "    \"\"\" Get files only for 6 categories (golden retriever, english setter,\n",
        "    german shepard, collie, pug) to test if the model works with a few breeds \"\"\"\n",
        "    # Take the path to directory \"dataset\"\n",
        "    path = os.getcwd()\n",
        "    path = path + \"/gdrive/My Drive/dataset\"\n",
        "    \n",
        "    path_pickle = os.getcwd() + \"/gdrive/My Drive/dogs_AI/pickles\"\n",
        "    \n",
        "    dataset_dirs = os.listdir(path)\n",
        "    # Declare the two list which will contain labels and path of the images\n",
        "    labels = list()\n",
        "    images = list()\n",
        "    \n",
        "    # counter for save batch every 1 000 images\n",
        "    i = 0\n",
        "    j = 1\n",
        "\n",
        "    # Browse the \"dataset\" directory to get all paths for images and labels which are the directories names\n",
        "    for dirs in dataset_dirs:\n",
        "        dir_path = path + \"/\" + dirs\n",
        "        for path2, dir, files in os.walk(dir_path):\n",
        "            if (dirs == \"pug\" or dirs == \"English_setter\" or dirs == \"German_shepherd\"\n",
        "                    or dirs == \"golden_retriever\" or dirs == \"collie\" or dirs == \"boxer\"):\n",
        "                for files_name in files:\n",
        "                    labels.append(categories.index(dirs))\n",
        "                    img = cv2_preprocess(dir_path + \"/\" + files_name, size, gray_scale)\n",
        "                    images.append(img.tolist())\n",
        "                    \n",
        "                    i += 1\n",
        "\n",
        "                    # Data augmentation\n",
        "                    if random.random() < 0.4:\n",
        "                        # flip the image\n",
        "                        img = np.fliplr(img)\n",
        "                        labels.append(categories.index(dirs))\n",
        "                        images.append(img.tolist())\n",
        "                        i += 1\n",
        "\n",
        "                    if random.random() < 0.2:\n",
        "                        img = data_left_translation(img, size)\n",
        "                        labels.append(categories.index(dirs))\n",
        "                        images.append(img.tolist())\n",
        "                        i += 1\n",
        "                    elif random.random() < 0.2:\n",
        "                        img = data_right_translation(img, size)\n",
        "                        labels.append(categories.index(dirs))\n",
        "                        images.append(img.tolist())\n",
        "                        i += 1\n",
        "                    \n",
        "                    img = None\n",
        "                    \n",
        "                    if i >= 400:\n",
        "                        pickle_out = open(\"{}/images_6_{}.pickle\".format(path_pickle, j), \"wb\")\n",
        "                        pickle.dump(images, pickle_out)\n",
        "                        pickle_out.close()\n",
        "\n",
        "                        pickle_out = open(\"{}/labels_6_{}.pickle\".format(path_pickle, j), \"wb\")\n",
        "                        pickle.dump(labels, pickle_out)\n",
        "                        pickle_out.close()\n",
        "                        \n",
        "                        images = None\n",
        "                        images = list()\n",
        "                        labels = None\n",
        "                        labels = list()\n",
        "                        \n",
        "                        i = 0\n",
        "                        print(\"end of j == {} in 6 breeds\".format(j))\n",
        "                        j += 1\n",
        "    \n",
        "    if i != 0:\n",
        "      pickle_out = open(\"{}/images_6_{}.pickle\".format(path_pickle, j), \"wb\")\n",
        "      pickle.dump(images, pickle_out)\n",
        "      pickle_out.close()\n",
        "\n",
        "      pickle_out = open(\"{}/labels_6_{}.pickle\".format(path_pickle, j), \"wb\")\n",
        "      pickle.dump(labels, pickle_out)\n",
        "      pickle_out.close()\n",
        "                        \n",
        "      images = None\n",
        "      labels = None\n",
        "                        \n",
        "      i = None\n",
        "      j = None\n",
        "      \n",
        "      print(\" end of 6 breeds\")\n",
        "    \n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VYzYyItBPEhC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def cv2_preprocess(path, size, gray_scale):\n",
        "    # channels = 1 if gray_scale else 3\n",
        "\n",
        "    if gray_scale:\n",
        "        img = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n",
        "    else:\n",
        "        img = cv2.imread(path)\n",
        "\n",
        "    img = cv2.resize(img, (size, size))\n",
        "    # img = np.array(img).reshape(-1, 192, 192, channels)\n",
        "    img = img / 255.0\n",
        "    return img\n",
        "\n",
        "\n",
        "def data_left_translation(img, size):\n",
        "    for i in range(size, 1, -1):\n",
        "        for j in range(size):\n",
        "            if i < size - 20:\n",
        "                img[j][i] = img[j][i-20]\n",
        "            elif i < size-1:\n",
        "                img[j][i] = 0\n",
        "    return img\n",
        "\n",
        "\n",
        "def data_right_translation(img, size):\n",
        "    for i in range(size, 1, -1):\n",
        "        for j in range(size):\n",
        "            if i < size - 20:\n",
        "                img[j][i] = img[j][i+20]\n",
        "    return img"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PTbUGoUb4t8p",
        "colab_type": "text"
      },
      "source": [
        "Launch the save for only 6 categories (be aware to choose good constants ofr only 6 categories)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uWiHA-Q4O8Ui",
        "colab_type": "code",
        "outputId": "b77f51e3-48bb-4cbf-a444-3970ebb1309b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "save_files_for_some_categories_cv2(get_categories_6_breeds(), SIZE, False)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "end of j == 1 in 6 breeds\n",
            "end of j == 2 in 6 breeds\n",
            "end of j == 3 in 6 breeds\n",
            "end of j == 4 in 6 breeds\n",
            " end of 6 breeds\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FMHGm_DX434t",
        "colab_type": "text"
      },
      "source": [
        "Fonction to save the model after the training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J0TEYoMu6IT_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def save_model(model, name):\n",
        "    \"\"\" Save a given model with a particular name.\n",
        "\n",
        "         Args:\n",
        "             model: the model to save\n",
        "             name: the name to use to save the model\n",
        "         \"\"\"\n",
        "    path = os.getcwd()\n",
        "    path = path + \"/gdrive/My Drive/dogs_AI/models/\" + name + \".model\"\n",
        "    model.save(path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tmep4mXR47ym",
        "colab_type": "text"
      },
      "source": [
        "To load every pickle files with pictures into a numpy array"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o_iuWR24a4lG",
        "colab_type": "code",
        "outputId": "a4e1c8ce-00f9-4508-e8a6-269fb29ed54a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "# better loading to test\n",
        "\n",
        "images_list = list()\n",
        "labels_list = list()\n",
        "labels = list()\n",
        "\n",
        "path_pickle = os.getcwd() + \"/gdrive/My Drive/dogs_AI/pickles/\"\n",
        "\n",
        "for i in range(5):\n",
        "  pickle_in = open(\"{}images_6_{}.pickle\".format(path_pickle, i+1), \"rb\")\n",
        "  images_list = pickle.load(pickle_in)\n",
        "  pickle_in.close()\n",
        "\n",
        "  pickle_in = open(\"{}labels_6_{}.pickle\".format(path_pickle, i+1), \"rb\")\n",
        "  labels_list = pickle.load(pickle_in)\n",
        "  pickle_in.close()\n",
        "  \n",
        "  images_list = np.array(images_list)\n",
        "  labels_list = np.array(labels_list)\n",
        "  \n",
        "  if i == 0:\n",
        "    images = np.array(images_list)\n",
        "    labels = np.array(labels_list)\n",
        "  else:\n",
        "    images = np.concatenate((images, images_list))\n",
        "    labels = np.concatenate((labels, labels_list))\n",
        "    \n",
        "  images_list = None\n",
        "  labels_list = None\n",
        "  \n",
        "  \n",
        "  print(i)\n",
        "  \n",
        "\n",
        "randomize = np.arange(len(labels))\n",
        "np.random.shuffle(randomize)\n",
        "images = images[randomize]\n",
        "labels = labels[randomize]\n",
        "\n",
        "images = images.reshape(-1, SIZE, SIZE, 3)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nU5ybqEU56gu",
        "colab_type": "text"
      },
      "source": [
        "To see the shape of the numpy array containing all the pictures"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C6ClMjIg2BiW",
        "colab_type": "code",
        "outputId": "271f85be-746a-4af7-f90a-5192a83d40f8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "print(images.shape)\n",
        "print(len(labels))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1698, 250, 250, 3)\n",
            "1698\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B8sQZVGn3Loq",
        "colab_type": "code",
        "outputId": "4bf53ea6-f160-4a4d-c53e-3bd346f1cbb9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "images = list()\n",
        "labels = list()\n",
        "\n",
        "path_pickle = os.getcwd() + \"/gdrive/My Drive/dogs_AI/pickles/\"\n",
        "\n",
        "for i in range(2):\n",
        "  pickle_in = open(\"{}images_6_{}.pickle\".format(path_pickle, i+1), \"rb\")\n",
        "  images = images + pickle.load(pickle_in)\n",
        "  pickle_in.close()\n",
        "\n",
        "  pickle_in = open(\"{}labels_6_{}.pickle\".format(path_pickle, i+1), \"rb\")\n",
        "  labels = labels + pickle.load(pickle_in)\n",
        "  pickle_in.close()\n",
        "  \n",
        "  print(i)\n",
        "  \n",
        "combine = list(zip(images, labels))\n",
        "random.shuffle(combine)\n",
        "images, labels = zip(*combine)\n",
        "combine = None\n",
        "images = np.array(images).reshape(-1, SIZE, SIZE, 3)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x3sSVwLXip3K",
        "colab_type": "code",
        "outputId": "38e2add0-9d89-4213-ac9f-be04e570240f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "images.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1906, 200, 200, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6jpUgR3O6C-i",
        "colab_type": "text"
      },
      "source": [
        "If there is an error, to empty all variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ywoFvqHvYrlY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "images = None\n",
        "labels = None\n",
        "model = None"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hBrRGZ2mOSA5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = None"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zncX4jsH47g_",
        "colab_type": "code",
        "outputId": "a5eea3cd-8415-4098-de4f-81ebdddd36c8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1394
        }
      },
      "source": [
        "model = train(batch_size = 20, epochs=40, model_name=\"tryNewPhotos\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 1524 samples, validate on 382 samples\n",
            "Epoch 1/40\n",
            "1524/1524 [==============================] - 7s 4ms/sample - loss: 1.8277 - acc: 0.2001 - val_loss: 1.8013 - val_acc: 0.1806\n",
            "Epoch 2/40\n",
            "1524/1524 [==============================] - 6s 4ms/sample - loss: 1.7923 - acc: 0.2211 - val_loss: 1.8072 - val_acc: 0.1806\n",
            "Epoch 3/40\n",
            "1524/1524 [==============================] - 6s 4ms/sample - loss: 1.7863 - acc: 0.2264 - val_loss: 1.7926 - val_acc: 0.1806\n",
            "Epoch 4/40\n",
            "1524/1524 [==============================] - 6s 4ms/sample - loss: 1.7872 - acc: 0.2251 - val_loss: 1.7852 - val_acc: 0.1859\n",
            "Epoch 5/40\n",
            "1524/1524 [==============================] - 6s 4ms/sample - loss: 1.7699 - acc: 0.2231 - val_loss: 1.8006 - val_acc: 0.1806\n",
            "Epoch 6/40\n",
            "1524/1524 [==============================] - 6s 4ms/sample - loss: 1.7789 - acc: 0.2395 - val_loss: 1.7918 - val_acc: 0.1806\n",
            "Epoch 7/40\n",
            "1524/1524 [==============================] - 6s 4ms/sample - loss: 1.7457 - acc: 0.2408 - val_loss: 1.7637 - val_acc: 0.2042\n",
            "Epoch 8/40\n",
            "1524/1524 [==============================] - 6s 4ms/sample - loss: 1.7258 - acc: 0.2638 - val_loss: 1.8362 - val_acc: 0.2199\n",
            "Epoch 9/40\n",
            "1524/1524 [==============================] - 6s 4ms/sample - loss: 1.6861 - acc: 0.2946 - val_loss: 1.7185 - val_acc: 0.2749\n",
            "Epoch 10/40\n",
            "1524/1524 [==============================] - 6s 4ms/sample - loss: 1.6537 - acc: 0.3235 - val_loss: 1.7162 - val_acc: 0.3246\n",
            "Epoch 11/40\n",
            "1524/1524 [==============================] - 6s 4ms/sample - loss: 1.6451 - acc: 0.3346 - val_loss: 1.6876 - val_acc: 0.3403\n",
            "Epoch 12/40\n",
            "1524/1524 [==============================] - 6s 4ms/sample - loss: 1.6293 - acc: 0.3451 - val_loss: 1.7117 - val_acc: 0.2880\n",
            "Epoch 13/40\n",
            "1524/1524 [==============================] - 6s 4ms/sample - loss: 1.5940 - acc: 0.3825 - val_loss: 1.6489 - val_acc: 0.3560\n",
            "Epoch 14/40\n",
            "1524/1524 [==============================] - 6s 4ms/sample - loss: 1.5462 - acc: 0.4029 - val_loss: 1.6117 - val_acc: 0.3508\n",
            "Epoch 15/40\n",
            "1524/1524 [==============================] - 6s 4ms/sample - loss: 1.4874 - acc: 0.4114 - val_loss: 1.6292 - val_acc: 0.3770\n",
            "Epoch 16/40\n",
            "1524/1524 [==============================] - 6s 4ms/sample - loss: 1.4473 - acc: 0.4285 - val_loss: 1.6280 - val_acc: 0.3403\n",
            "Epoch 17/40\n",
            "1524/1524 [==============================] - 6s 4ms/sample - loss: 1.4120 - acc: 0.4436 - val_loss: 1.5808 - val_acc: 0.3848\n",
            "Epoch 18/40\n",
            "1524/1524 [==============================] - 6s 4ms/sample - loss: 1.3642 - acc: 0.4803 - val_loss: 1.6154 - val_acc: 0.3796\n",
            "Epoch 19/40\n",
            "1524/1524 [==============================] - 6s 4ms/sample - loss: 1.2572 - acc: 0.5033 - val_loss: 1.5994 - val_acc: 0.3403\n",
            "Epoch 20/40\n",
            "1524/1524 [==============================] - 6s 4ms/sample - loss: 1.2189 - acc: 0.5348 - val_loss: 1.6551 - val_acc: 0.3901\n",
            "Epoch 21/40\n",
            "1524/1524 [==============================] - 6s 4ms/sample - loss: 1.1322 - acc: 0.5689 - val_loss: 1.6500 - val_acc: 0.4293\n",
            "Epoch 22/40\n",
            "1524/1524 [==============================] - 6s 4ms/sample - loss: 1.1054 - acc: 0.6024 - val_loss: 1.7194 - val_acc: 0.3822\n",
            "Epoch 23/40\n",
            "1524/1524 [==============================] - 6s 4ms/sample - loss: 0.9615 - acc: 0.6194 - val_loss: 1.6931 - val_acc: 0.4110\n",
            "Epoch 24/40\n",
            "1524/1524 [==============================] - 6s 4ms/sample - loss: 0.9427 - acc: 0.6417 - val_loss: 1.7563 - val_acc: 0.4476\n",
            "Epoch 25/40\n",
            "1524/1524 [==============================] - 6s 4ms/sample - loss: 0.8832 - acc: 0.6608 - val_loss: 1.7741 - val_acc: 0.4162\n",
            "Epoch 26/40\n",
            "1524/1524 [==============================] - 6s 4ms/sample - loss: 0.7964 - acc: 0.7054 - val_loss: 1.9035 - val_acc: 0.4267\n",
            "Epoch 27/40\n",
            "1524/1524 [==============================] - 6s 4ms/sample - loss: 0.7718 - acc: 0.7224 - val_loss: 1.7728 - val_acc: 0.3848\n",
            "Epoch 28/40\n",
            "1524/1524 [==============================] - 6s 4ms/sample - loss: 0.6564 - acc: 0.7723 - val_loss: 2.0310 - val_acc: 0.4084\n",
            "Epoch 29/40\n",
            "1524/1524 [==============================] - 6s 4ms/sample - loss: 0.6573 - acc: 0.7585 - val_loss: 1.9599 - val_acc: 0.4005\n",
            "Epoch 30/40\n",
            "1524/1524 [==============================] - 6s 4ms/sample - loss: 0.5890 - acc: 0.7946 - val_loss: 2.2314 - val_acc: 0.3874\n",
            "Epoch 31/40\n",
            "1524/1524 [==============================] - 6s 4ms/sample - loss: 0.5835 - acc: 0.7979 - val_loss: 2.1577 - val_acc: 0.3927\n",
            "Epoch 32/40\n",
            "1524/1524 [==============================] - 6s 4ms/sample - loss: 0.5154 - acc: 0.8222 - val_loss: 2.3565 - val_acc: 0.3691\n",
            "Epoch 33/40\n",
            "1524/1524 [==============================] - 6s 4ms/sample - loss: 0.4847 - acc: 0.8314 - val_loss: 2.4453 - val_acc: 0.3953\n",
            "Epoch 34/40\n",
            "1524/1524 [==============================] - 6s 4ms/sample - loss: 0.4639 - acc: 0.8425 - val_loss: 2.4040 - val_acc: 0.3927\n",
            "Epoch 35/40\n",
            "1524/1524 [==============================] - 6s 4ms/sample - loss: 0.4256 - acc: 0.8517 - val_loss: 2.5097 - val_acc: 0.4293\n",
            "Epoch 36/40\n",
            "1524/1524 [==============================] - 6s 4ms/sample - loss: 0.3951 - acc: 0.8681 - val_loss: 2.7711 - val_acc: 0.3822\n",
            "Epoch 37/40\n",
            "1524/1524 [==============================] - 6s 4ms/sample - loss: 0.3677 - acc: 0.8806 - val_loss: 2.8425 - val_acc: 0.4162\n",
            "Epoch 38/40\n",
            "1524/1524 [==============================] - 6s 4ms/sample - loss: 0.3502 - acc: 0.8845 - val_loss: 2.6695 - val_acc: 0.3979\n",
            "Epoch 39/40\n",
            "1524/1524 [==============================] - 6s 4ms/sample - loss: 0.3690 - acc: 0.8806 - val_loss: 2.5413 - val_acc: 0.4110\n",
            "Epoch 40/40\n",
            "1524/1524 [==============================] - 6s 4ms/sample - loss: 0.3080 - acc: 0.8898 - val_loss: 2.7407 - val_acc: 0.3979\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NQKV1QYv6MaL",
        "colab_type": "text"
      },
      "source": [
        "Normal training function with test with convutional layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OxP4g9n5D0gU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " # training function\n",
        "def train(gray_scale=False, epochs=10, batch_size=32, model_name=\"dogs_colab\", dogs_6=False):\n",
        "    \"\"\" Train the model with data contained in the dataset directory.\n",
        "\n",
        "    Args:\n",
        "        learning_rate: the learning rate used by the optimizer of the model\n",
        "        gray_scale: boolean to know if we need to use grayscale image for inputs\n",
        "        epochs: number of epochs to train the model, 5 by default\n",
        "        batch_size: the batch size for data to train the model, 20 by default\n",
        "        steps_per_epoch: number of steps per epoch, 100 by default\n",
        "        model_name: the name used to save the model, dogs by default\n",
        "        dogs_6: boolean to know if we need to use only 6 breeds to test the model\n",
        "    Return:\n",
        "        a trained model\n",
        "    \"\"\"\n",
        "\n",
        "    model = km.Sequential()\n",
        "    model.add(layers.Conv2D(32, (4, 4), input_shape=(SIZE, SIZE, 3), activation=\"relu\"))\n",
        "    model.add(layers.MaxPooling2D(pool_size=(2, 2), data_format='channels_last'))\n",
        "    model.add(layers.Dropout(0.3))\n",
        "\n",
        "    model.add(layers.Conv2D(64, (4, 4), activation=\"relu\"))\n",
        "    model.add(layers.MaxPooling2D(pool_size=(2, 2), data_format='channels_last'))\n",
        "    model.add(layers.Dropout(0.3))\n",
        "    \n",
        "    model.add(layers.Conv2D(128, (4, 4), activation=\"relu\"))\n",
        "    model.add(layers.MaxPooling2D(pool_size=(2, 2), data_format='channels_last'))\n",
        "    model.add(layers.Dropout(0.3))\n",
        "    \n",
        "    model.add(layers.Conv2D(256, (4, 4), activation=\"relu\"))\n",
        "    model.add(layers.MaxPooling2D(pool_size=(2, 2), data_format='channels_last'))\n",
        "    model.add(layers.Dropout(0.3))\n",
        "\n",
        "    model.add(layers.Flatten())\n",
        "    model.add(layers.Dense(100, activation=\"relu\"))\n",
        "    model.add(layers.Dense(50, activation=\"relu\"))\n",
        "    model.add(layers.Dropout(0.3))\n",
        "\n",
        "    model.add(layers.Dense(len(CATEGORIES), activation=\"softmax\", kernel_regularizer=kr.l2(0.001)))\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=\"adam\",\n",
        "        loss='sparse_categorical_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "\n",
        "    model.fit(images, labels, batch_size=batch_size, epochs=epochs, validation_split=0.2)\n",
        "\n",
        "    save_model(model, model_name)\n",
        "\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DAvfQd4W6RKo",
        "colab_type": "text"
      },
      "source": [
        "Same but with a tensorboard option"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R13d3bdEFIgt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# training function for optimization with tensorboard\n",
        "def train_tensorboard(conv, dense, neurons, regularization, epochs=10, batch_size=32, model_name=\"dogs\", dogs_6=False):\n",
        "    \"\"\" Train the model with data contained in the dataset directory.\n",
        "\n",
        "    Args:\n",
        "        learning_rate: the learning rate used by the optimizer of the model\n",
        "        gray_scale: boolean to know if we need to use grayscale image for inputs\n",
        "        epochs: number of epochs to train the model, 5 by default\n",
        "        batch_size: the batch size for data to train the model, 20 by default\n",
        "        steps_per_epoch: number of steps per epoch, 100 by default\n",
        "        model_name: the name used to save the model, dogs by default\n",
        "        dogs_6: boolean to know if we need to use only 6 breeds to test the model\n",
        "    Return:\n",
        "        a trained model\n",
        "    \"\"\"\n",
        "    \n",
        "    clear_output()\n",
        "    print(model_name)\n",
        "    \n",
        "    model = km.Sequential()\n",
        "    model.add(layers.Conv2D(32, (4, 4), input_shape=(SIZE, SIZE, 3)))\n",
        "    model.add(layers.MaxPooling2D(pool_size=(2, 2), data_format=\"channels_last\"))\n",
        "    model.add(layers.Activation(\"relu\"))\n",
        "    \n",
        "    for l in range(conv-1):\n",
        "      model.add(layers.Conv2D(32*(l+1), (4, 4)))\n",
        "      model.add(layers.Activation(\"relu\"))\n",
        "      model.add(layers.MaxPooling2D(pool_size=(2, 2), data_format=\"channels_last\"))\n",
        "      model.add(layers.Dropout(0.2))\n",
        "  \n",
        "                \n",
        "    for l in range(dense):\n",
        "      model.add(layers.Flatten())\n",
        "      model.add(layers.Dense(neurons*(dense+1-l), activation=\"relu\"))\n",
        "      model.add(layers.Dropout(0.3))\n",
        "    \n",
        "\n",
        "    model.add(layers.Flatten())\n",
        "    model.add(layers.Dense(len(CATEGORIES), activation=\"softmax\", kernel_regularizer=kr.l2(regularization)))\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=\"adam\",\n",
        "        loss='sparse_categorical_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "    \n",
        "    model.fit(images, labels, batch_size=batch_size, epochs=epochs, validation_split=0.2, callbacks=[tensorboard])\n",
        "\n",
        "    save_model(model, model_name)\n",
        "\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zwQBlqLJ6VFA",
        "colab_type": "text"
      },
      "source": [
        "Loop to test different options"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KHO-Vy4oDszZ",
        "colab_type": "code",
        "outputId": "023d22ea-7316-46ce-c473-8684cf4ab35b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        }
      },
      "source": [
        "path_log = os.getcwd() + \"/gdrive/My Drive/dogs_AI/logs/try2/\"\n",
        "\n",
        "\n",
        "\n",
        "# listes pour changer l'entrainement\n",
        "regularization = [0.0001, 0.001]\n",
        "conv2D_nb = [2, 3, 4, 5]\n",
        "dense_nb = [1, 2, 3]\n",
        "neurons_dense = [50, 100, 200]\n",
        "# boucle for pour entrainer\n",
        "for neurons in neurons_dense:\n",
        "  for dense in dense_nb:\n",
        "    for conv in conv2D_nb:\n",
        "      for reg in regularization:\n",
        "        name_log = \"log_conv={}_dense={}_neurons={}_regu={}\".format(conv, dense, neurons, reg)\n",
        "\n",
        "        # lancement tensorboard\n",
        "        tensorboard = call.TensorBoard(log_dir=path_log + '{}'.format(name_log))\n",
        "\n",
        "        train_tensorboard(conv, dense, neurons, reg, model_name=name_log, batch_size=18, epochs=15)\n",
        "      "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "log_conv=2_dense=3_neurons=50_regu=0.0001\n",
            "Train on 1524 samples, validate on 382 samples\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-46-e3f07c8fc8d9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mtensorboard\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensorBoard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath_log\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_log\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mtrain_tensorboard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdense\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneurons\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname_log\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m18\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-45-140458f61d99>\u001b[0m in \u001b[0;36mtrain_tensorboard\u001b[0;34m(conv, dense, neurons, regularization, epochs, batch_size, model_name, dogs_6)\u001b[0m\n\u001b[1;32m     44\u001b[0m     )\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtensorboard\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0msave_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    878\u001b[0m           \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m           \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 880\u001b[0;31m           validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m    881\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m   def evaluate(self,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, mode, validation_in_fit, **kwargs)\u001b[0m\n\u001b[1;32m    213\u001b[0m       \u001b[0msamples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_samples_or_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m       \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Handle ProgBarLogger separately in this loop.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 215\u001b[0;31m       mode=mode)\n\u001b[0m\u001b[1;32m    216\u001b[0m   \u001b[0;31m# TODO(omalleyt): Handle ProgBar as part of Callbacks once hooks are ready.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m   \u001b[0mprogbar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraining_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_progbar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcount_mode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36mconfigure_callbacks\u001b[0;34m(callbacks, model, do_validation, batch_size, epochs, steps_per_epoch, samples, verbose, count_mode, mode)\u001b[0m\n\u001b[1;32m    104\u001b[0m   \u001b[0;31m# Set callback model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m   \u001b[0mcallback_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_callback_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m   \u001b[0mcallback_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallback_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m   \u001b[0;31m# Set callback parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36mset_model\u001b[0;34m(self, model)\u001b[0m\n\u001b[1;32m    176\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m       \u001b[0mcallback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_batch_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36mset_model\u001b[0;34m(self, model)\u001b[0m\n\u001b[1;32m   1008\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1009\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1010\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_writer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1011\u001b[0m     \u001b[0;31m# histogram summaries only enabled in graph mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1012\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36m_init_writer\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    945\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwriter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msummary_ops_v2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_file_writer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    946\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite_graph\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 947\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwriter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_summary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFileWriter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    948\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwriter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_summary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFileWriter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36mget_session\u001b[0;34m()\u001b[0m\n\u001b[1;32m    480\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_MANUAL_VAR_INIT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m       \u001b[0m_initialize_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m_initialize_variables\u001b[0;34m(session)\u001b[0m\n\u001b[1;32m    756\u001b[0m     \u001b[0;31m# marked as initialized.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    757\u001b[0m     is_initialized = session.run(\n\u001b[0;32m--> 758\u001b[0;31m         [variables_module.is_variable_initialized(v) for v in candidate_vars])\n\u001b[0m\u001b[1;32m    759\u001b[0m     \u001b[0muninitialized_vars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    760\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mflag\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_initialized\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcandidate_vars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 929\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1150\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1152\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1153\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1328\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1329\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1332\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1333\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1334\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1335\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1336\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1315\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m       \u001b[0;31m# Ensure any changes to the graph are reflected in the runtime.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1317\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m   1319\u001b[0m           options, feed_dict, fetch_list, target_list, run_metadata)\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_extend_graph\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1350\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1351\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session_run_lock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1352\u001b[0;31m       \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExtendSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1353\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1354\u001b[0m   \u001b[0;31m# The threshold to run garbage collection to delete dead tensors.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zWH-L7f0Oqmb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "files.download('images.pickle')\n",
        "files.download('labels.pickle')\n",
        "files.download('images_6.pickle')\n",
        "files.download('labels_6.pickle')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p5EgAK8v6dTn",
        "colab_type": "text"
      },
      "source": [
        "Function to load a model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dMnXO2ypTUr0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_model(name):\n",
        "  path = os.getcwd() + \"/gdrive/My Drive/dogs_AI/models/\" + name + \".model\"\n",
        "  return km.load_model(path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CQogDVsT6fwW",
        "colab_type": "text"
      },
      "source": [
        "To test a model on a test photo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eUelfn5HSZ-c",
        "colab_type": "code",
        "outputId": "e3dcdaf2-c2ab-4156-b8e2-b28aaacccb69",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "model = load_model(\"tryNewPhotos\")\n",
        "\n",
        "path = os.getcwd() + \"/gdrive/My Drive/dogs_AI/photo_test/blago.jpg\"\n",
        "\n",
        "image = cv2_preprocess(path, SIZE, False)\n",
        "# print(image)\n",
        "image = np.array(image).reshape(1, SIZE, SIZE, 3)\n",
        "\n",
        "prediction = model.predict([image])\n",
        "\n",
        "prediction1 = np.argmax(prediction)\n",
        "prediction1_pourc = round(prediction[0][prediction1]*100, 2)\n",
        "prediction1 = CATEGORIES[prediction1]\n",
        "print(\"{} à {}%\".format(prediction1, prediction1_pourc))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "collie à 93.71%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NpWrNEaHVQR2",
        "colab_type": "code",
        "outputId": "6b3bbab0-938a-403d-9106-6ff26c81cd33",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "prediction*100"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0.5988873 ,  4.4927077 ,  0.33432558,  0.2872912 , 93.712845  ,\n",
              "         0.57394195]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OWcjkACqVuoE",
        "colab_type": "code",
        "outputId": "b08b21e4-8aef-4c6b-f8d7-c3a519807aea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "CATEGORIES"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['pug',\n",
              " 'English_setter',\n",
              " 'golden_retriever',\n",
              " 'German_shepherd',\n",
              " 'collie',\n",
              " 'boxer']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ChllxaWeK28Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# transfer learning with Inception\n",
        "\n",
        "def train_inception(dense, neurons, regularization, epochs=10, batch_size=32, model_name=\"dogs_inceptionV3\", dogs_6=False):\n",
        "    \"\"\" Train the model with data contained in the dataset directory.\n",
        "\n",
        "    Args:\n",
        "        learning_rate: the learning rate used by the optimizer of the model\n",
        "        gray_scale: boolean to know if we need to use grayscale image for inputs\n",
        "        epochs: number of epochs to train the model, 5 by default\n",
        "        batch_size: the batch size for data to train the model, 20 by default\n",
        "        steps_per_epoch: number of steps per epoch, 100 by default\n",
        "        model_name: the name used to save the model, dogs by default\n",
        "        dogs_6: boolean to know if we need to use only 6 breeds to test the model\n",
        "    Return:\n",
        "        a trained model\n",
        "    \"\"\"\n",
        "    \n",
        "    clear_output()\n",
        "    print(model_name)\n",
        "    \n",
        "    conv_base = ka.InceptionV3(include_top=False, weights=\"imagenet\", input_shape=(SIZE, SIZE, 3))\n",
        "    \n",
        "    model = km.Sequential()\n",
        "    model.add(conv_base)\n",
        "    model.add(layers.GlobalAveragePooling2D())\n",
        "    \n",
        "    # model.add(layers.Flatten())\n",
        "  \n",
        "                \n",
        "    # for l in range(dense):\n",
        "    #   model.add(layers.Dense(neurons*(dense+1-l), activation=\"relu\"))\n",
        "    #   model.add(layers.Dropout(0.3))\n",
        "      \n",
        "    \n",
        "    model.add(layers.Dense(len(CATEGORIES), activation=\"softmax\", kernel_regularizer=kr.l2(regularization)))\n",
        "    \n",
        "    model.summary()\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=\"adam\",\n",
        "        loss='sparse_categorical_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "    \n",
        "    model.fit(images, labels, batch_size=batch_size, epochs=epochs, validation_split=0.2)\n",
        "\n",
        "    save_model(model, model_name)\n",
        "\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FYsPJ3X16pdk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# transfer learning with ResNet\n",
        "\n",
        "def train_resnet(dense, neurons, regularization, epochs=10, batch_size=32, model_name=\"dogs_inceptionV3\", dogs_6=False):\n",
        "    \"\"\" Train the model with data contained in the dataset directory.\n",
        "\n",
        "    Args:\n",
        "        learning_rate: the learning rate used by the optimizer of the model\n",
        "        gray_scale: boolean to know if we need to use grayscale image for inputs\n",
        "        epochs: number of epochs to train the model, 5 by default\n",
        "        batch_size: the batch size for data to train the model, 20 by default\n",
        "        steps_per_epoch: number of steps per epoch, 100 by default\n",
        "        model_name: the name used to save the model, dogs by default\n",
        "        dogs_6: boolean to know if we need to use only 6 breeds to test the model\n",
        "    Return:\n",
        "        a trained model\n",
        "    \"\"\"\n",
        "    \n",
        "    clear_output()\n",
        "    print(model_name)\n",
        "    \n",
        "    conv_base = ka.InceptionResNetV2(include_top=False, weights=\"imagenet\", input_shape=(SIZE, SIZE, 3))\n",
        "    \n",
        "    model = km.Sequential()\n",
        "    model.add(conv_base)\n",
        "    model.add(layers.GlobalAveragePooling2D())\n",
        "    \n",
        "    # model.add(layers.Flatten())\n",
        "  \n",
        "                \n",
        "    # for l in range(dense):\n",
        "    #   model.add(layers.Dense(neurons*(dense+1-l), activation=\"relu\"))\n",
        "    #   model.add(layers.Dropout(0.3))\n",
        "      \n",
        "    \n",
        "    model.add(layers.Dense(len(CATEGORIES), activation=\"softmax\", kernel_regularizer=kr.l2(regularization)))\n",
        "    \n",
        "    model.summary()\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=\"adam\",\n",
        "        loss='sparse_categorical_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "    \n",
        "    model.fit(images, labels, batch_size=batch_size, epochs=epochs, validation_split=0.2)\n",
        "\n",
        "    save_model(model, model_name)\n",
        "\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IqBrxXLtFnVG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# transfer learning with Inception\n",
        "\n",
        "# training function for optimization with tensorboard\n",
        "def train_inception_tensorboard(dense, neurons, regularization, epochs=10, batch_size=32, model_name=\"dogs_inceptionV3\", dogs_6=False):\n",
        "    \"\"\" Train the model with data contained in the dataset directory.\n",
        "\n",
        "    Args:\n",
        "        learning_rate: the learning rate used by the optimizer of the model\n",
        "        gray_scale: boolean to know if we need to use grayscale image for inputs\n",
        "        epochs: number of epochs to train the model, 5 by default\n",
        "        batch_size: the batch size for data to train the model, 20 by default\n",
        "        steps_per_epoch: number of steps per epoch, 100 by default\n",
        "        model_name: the name used to save the model, dogs by default\n",
        "        dogs_6: boolean to know if we need to use only 6 breeds to test the model\n",
        "    Return:\n",
        "        a trained model\n",
        "    \"\"\"\n",
        "    \n",
        "    clear_output()\n",
        "    print(model_name)\n",
        "    \n",
        "    conv_base = ka.InceptionV3(include_top=False, weights=\"imagenet\", input_shape=(SIZE, SIZE, 3))\n",
        "    \n",
        "    model = km.Sequential()\n",
        "    model.add(conv_base)\n",
        "    model.add(layers.GlobalAveragePooling2D())\n",
        "    \n",
        "    model.add(layers.Flatten())\n",
        "  \n",
        "                \n",
        "    for l in range(dense):\n",
        "      model.add(layers.Dense(neurons*(dense+1-l), activation=\"relu\"))\n",
        "      model.add(layers.Dropout(0.3))\n",
        "      \n",
        "    \n",
        "    model.add(layers.Dense(len(CATEGORIES), activation=\"softmax\", kernel_regularizer=kr.l2(regularization)))\n",
        "    \n",
        "    model.summary()\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=\"adam\",\n",
        "        loss='sparse_categorical_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "    \n",
        "    model.fit(images, labels, batch_size=batch_size, epochs=epochs, validation_split=0.3, callbacks=[tensorboard])\n",
        "\n",
        "    save_model(model, model_name)\n",
        "\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PrgLoJtAp2B5",
        "colab_type": "code",
        "outputId": "d99922b2-c30d-498f-aee1-cfc3e32ae6c4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "dense = 1\n",
        "neurons = 100\n",
        "reg = 0.00004\n",
        "name_log = \"log_dense={}_neurons={}_regu={}\".format(dense, neurons, reg)\n",
        "\n",
        "train_inception(dense, neurons, reg, model_name=name_log, batch_size=10, epochs=20)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "log_dense=1_neurons=100_regu=4e-05\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.5/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "87916544/87910968 [==============================] - 3s 0us/step\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "inception_v3 (Model)         (None, 6, 6, 2048)        21802784  \n",
            "_________________________________________________________________\n",
            "global_average_pooling2d (Gl (None, 2048)              0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 6)                 12294     \n",
            "=================================================================\n",
            "Total params: 21,815,078\n",
            "Trainable params: 21,780,646\n",
            "Non-trainable params: 34,432\n",
            "_________________________________________________________________\n",
            "Train on 1358 samples, validate on 340 samples\n",
            "Epoch 1/20\n",
            "1358/1358 [==============================] - 61s 45ms/sample - loss: 1.2922 - acc: 0.5221 - val_loss: 3.8175 - val_acc: 0.2647\n",
            "Epoch 2/20\n",
            "1358/1358 [==============================] - 46s 34ms/sample - loss: 1.0565 - acc: 0.6053 - val_loss: 5.6841 - val_acc: 0.3382\n",
            "Epoch 3/20\n",
            "1358/1358 [==============================] - 46s 34ms/sample - loss: 0.8840 - acc: 0.6937 - val_loss: 2.3170 - val_acc: 0.4824\n",
            "Epoch 4/20\n",
            "1358/1358 [==============================] - 46s 34ms/sample - loss: 0.7540 - acc: 0.7371 - val_loss: 2.8393 - val_acc: 0.3294\n",
            "Epoch 5/20\n",
            "1358/1358 [==============================] - 46s 34ms/sample - loss: 0.6440 - acc: 0.7680 - val_loss: 1.6730 - val_acc: 0.4588\n",
            "Epoch 6/20\n",
            "1358/1358 [==============================] - 46s 34ms/sample - loss: 0.5196 - acc: 0.8240 - val_loss: 1.3368 - val_acc: 0.5471\n",
            "Epoch 7/20\n",
            "1358/1358 [==============================] - 46s 34ms/sample - loss: 0.4901 - acc: 0.8159 - val_loss: 1.0521 - val_acc: 0.5941\n",
            "Epoch 8/20\n",
            "1358/1358 [==============================] - 47s 34ms/sample - loss: 0.4133 - acc: 0.8638 - val_loss: 1.4199 - val_acc: 0.5735\n",
            "Epoch 9/20\n",
            "1358/1358 [==============================] - 47s 34ms/sample - loss: 0.5448 - acc: 0.8049 - val_loss: 3.9390 - val_acc: 0.3353\n",
            "Epoch 10/20\n",
            "1358/1358 [==============================] - 47s 34ms/sample - loss: 0.4469 - acc: 0.8395 - val_loss: 0.8792 - val_acc: 0.6882\n",
            "Epoch 11/20\n",
            "1358/1358 [==============================] - 47s 34ms/sample - loss: 0.3405 - acc: 0.8814 - val_loss: 1.2102 - val_acc: 0.6706\n",
            "Epoch 12/20\n",
            "1358/1358 [==============================] - 46s 34ms/sample - loss: 0.3182 - acc: 0.8932 - val_loss: 1.3367 - val_acc: 0.6088\n",
            "Epoch 13/20\n",
            "1358/1358 [==============================] - 47s 34ms/sample - loss: 0.3279 - acc: 0.8969 - val_loss: 1.0725 - val_acc: 0.6382\n",
            "Epoch 14/20\n",
            "1358/1358 [==============================] - 46s 34ms/sample - loss: 0.3051 - acc: 0.8976 - val_loss: 1.0295 - val_acc: 0.6912\n",
            "Epoch 15/20\n",
            "1358/1358 [==============================] - 46s 34ms/sample - loss: 0.2137 - acc: 0.9271 - val_loss: 2.0986 - val_acc: 0.5559\n",
            "Epoch 16/20\n",
            "1358/1358 [==============================] - 46s 34ms/sample - loss: 0.2573 - acc: 0.9124 - val_loss: 1.0999 - val_acc: 0.6794\n",
            "Epoch 17/20\n",
            "1358/1358 [==============================] - 46s 34ms/sample - loss: 0.2281 - acc: 0.9197 - val_loss: 1.9870 - val_acc: 0.4794\n",
            "Epoch 18/20\n",
            "1358/1358 [==============================] - 46s 34ms/sample - loss: 0.4211 - acc: 0.8689 - val_loss: 2.2870 - val_acc: 0.4971\n",
            "Epoch 19/20\n",
            "1358/1358 [==============================] - 46s 34ms/sample - loss: 0.2120 - acc: 0.9337 - val_loss: 0.8439 - val_acc: 0.7382\n",
            "Epoch 20/20\n",
            "1358/1358 [==============================] - 46s 34ms/sample - loss: 0.1402 - acc: 0.9566 - val_loss: 1.3771 - val_acc: 0.6765\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.engine.sequential.Sequential at 0x7fd58df61f60>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nvDcGo-f7MT2",
        "colab_type": "code",
        "outputId": "132e4d63-e8c7-46f4-9e3f-1cf2c8c215b5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "dense = 1\n",
        "neurons = 100\n",
        "reg = 0.000005\n",
        "name_log = \"log_dense={}_neurons={}_regu={}\".format(dense, neurons, reg)\n",
        "\n",
        "train_resnet(dense, neurons, reg, model_name=name_log, batch_size=10, epochs=20)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "log_dense=1_neurons=100_regu=1e-05\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XVr8GlxMp_PW",
        "colab_type": "code",
        "outputId": "51bede7d-6a28-464d-e585-81979023dd2e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Try multiple args for Inception Model\n",
        "\n",
        "path_log = os.getcwd() + \"/gdrive/My Drive/dogs_AI/logs/tryIncep2/\"\n",
        "\n",
        "\n",
        "\n",
        "# listes pour changer l'entrainement\n",
        "regularization = [0.0001, 0.0003, 0.0005, 0.001]\n",
        "dense_nb = [1]\n",
        "neurons_dense = [100, 120]\n",
        "# boucle for pour entrainer\n",
        "for neurons in neurons_dense:\n",
        "  for dense in dense_nb:\n",
        "      for reg in regularization:\n",
        "        name_log = \"log_dense={}_neurons={}_regu={}\".format(dense, neurons, reg)\n",
        "\n",
        "        # lancement tensorboard\n",
        "        tensorboard = call.TensorBoard(log_dir=path_log + '{}'.format(name_log))\n",
        "\n",
        "        train_inception_tensorboard(dense, neurons, reg, model_name=name_log, batch_size=20, epochs=25)\n",
        "        \n",
        "        model = None"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "log_dense=2_neurons=100_regu=0.0005\n",
            "Model: \"sequential_11\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "inception_v3 (Model)         (None, 4, 4, 2048)        21802784  \n",
            "_________________________________________________________________\n",
            "global_average_pooling2d_11  (None, 2048)              0         \n",
            "_________________________________________________________________\n",
            "flatten_11 (Flatten)         (None, 2048)              0         \n",
            "_________________________________________________________________\n",
            "dense_27 (Dense)             (None, 300)               614700    \n",
            "_________________________________________________________________\n",
            "dropout_16 (Dropout)         (None, 300)               0         \n",
            "_________________________________________________________________\n",
            "dense_28 (Dense)             (None, 200)               60200     \n",
            "_________________________________________________________________\n",
            "dropout_17 (Dropout)         (None, 200)               0         \n",
            "_________________________________________________________________\n",
            "dense_29 (Dense)             (None, 6)                 1206      \n",
            "=================================================================\n",
            "Total params: 22,478,890\n",
            "Trainable params: 22,444,458\n",
            "Non-trainable params: 34,432\n",
            "_________________________________________________________________\n",
            "Train on 1524 samples, validate on 382 samples\n",
            "Epoch 1/20\n",
            "1524/1524 [==============================] - 35s 23ms/sample - loss: 1.5169 - acc: 0.4350 - val_loss: 5.4148 - val_acc: 0.1702\n",
            "Epoch 2/20\n",
            "1524/1524 [==============================] - 14s 9ms/sample - loss: 1.1927 - acc: 0.5702 - val_loss: 13.9268 - val_acc: 0.1361\n",
            "Epoch 3/20\n",
            "1524/1524 [==============================] - 14s 9ms/sample - loss: 1.0594 - acc: 0.6411 - val_loss: 25.8969 - val_acc: 0.1649\n",
            "Epoch 4/20\n",
            "1524/1524 [==============================] - 14s 9ms/sample - loss: 1.0229 - acc: 0.6509 - val_loss: 4.9300 - val_acc: 0.2487\n",
            "Epoch 5/20\n",
            "1524/1524 [==============================] - 14s 9ms/sample - loss: 0.8557 - acc: 0.7106 - val_loss: 1.6763 - val_acc: 0.4738\n",
            "Epoch 6/20\n",
            "1524/1524 [==============================] - 14s 9ms/sample - loss: 1.0034 - acc: 0.6686 - val_loss: 12.7794 - val_acc: 0.2435\n",
            "Epoch 7/20\n",
            "1524/1524 [==============================] - 14s 9ms/sample - loss: 0.7902 - acc: 0.7474 - val_loss: 2.1691 - val_acc: 0.5393\n",
            "Epoch 8/20\n",
            "1524/1524 [==============================] - 14s 9ms/sample - loss: 0.6012 - acc: 0.8005 - val_loss: 4.8022 - val_acc: 0.3220\n",
            "Epoch 9/20\n",
            "1524/1524 [==============================] - 14s 9ms/sample - loss: 0.6936 - acc: 0.7848 - val_loss: 1.0394 - val_acc: 0.6832\n",
            "Epoch 10/20\n",
            "1524/1524 [==============================] - 14s 9ms/sample - loss: 0.5046 - acc: 0.8346 - val_loss: 0.9791 - val_acc: 0.6754\n",
            "Epoch 11/20\n",
            "1524/1524 [==============================] - 14s 9ms/sample - loss: 0.4684 - acc: 0.8451 - val_loss: 1.4416 - val_acc: 0.6440\n",
            "Epoch 12/20\n",
            "1524/1524 [==============================] - 14s 9ms/sample - loss: 0.4332 - acc: 0.8602 - val_loss: 0.9480 - val_acc: 0.7120\n",
            "Epoch 13/20\n",
            "1524/1524 [==============================] - 14s 9ms/sample - loss: 0.4211 - acc: 0.8596 - val_loss: 2.8188 - val_acc: 0.4921\n",
            "Epoch 14/20\n",
            "1524/1524 [==============================] - 14s 9ms/sample - loss: 0.4538 - acc: 0.8458 - val_loss: 1.9871 - val_acc: 0.6414\n",
            "Epoch 15/20\n",
            "1524/1524 [==============================] - 14s 9ms/sample - loss: 0.4731 - acc: 0.8379 - val_loss: 1.6080 - val_acc: 0.5733\n",
            "Epoch 16/20\n",
            "1524/1524 [==============================] - 14s 9ms/sample - loss: 0.4280 - acc: 0.8622 - val_loss: 1.1860 - val_acc: 0.6047\n",
            "Epoch 17/20\n",
            "1524/1524 [==============================] - 14s 9ms/sample - loss: 0.4149 - acc: 0.8694 - val_loss: 1.7690 - val_acc: 0.4738\n",
            "Epoch 18/20\n",
            "1524/1524 [==============================] - 14s 9ms/sample - loss: 0.4652 - acc: 0.8478 - val_loss: 1.0442 - val_acc: 0.7016\n",
            "Epoch 19/20\n",
            "1524/1524 [==============================] - 14s 9ms/sample - loss: 0.5380 - acc: 0.8307 - val_loss: 1.0853 - val_acc: 0.6335\n",
            "Epoch 20/20\n",
            "1524/1524 [==============================] - 14s 9ms/sample - loss: 0.3461 - acc: 0.8793 - val_loss: 1.1066 - val_acc: 0.6545\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}